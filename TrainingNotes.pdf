The Shared Location would be https://tinyurl.com/airtel1-dec19

Venkat
venkatraji71@gmail.com
9821422745

The download location for the training stuff would be :- https://drive.google.com/open?id=1s0dGWPwTuu-unGx9xJhTdfnf47J24yDp

The credentials for logging in to the image would be :
UserName: notroot
Password: hadoop123

Timings for the training

a)    Start at 9.00
b)    First Break at 11.00 → 15 min
c)    Lunch Break at 1.00 →  45 min
d)    Second Break at 2.45 → 15 min 
Day 1

Prerequisites for the training

a)    Eclipse in your system
b)    How to deploy this in the image → Hadoop + Apache Spark is already present in the image
c)    We can of course work individually in our local systems also without hadoop.

Spark

a)    Spark Core → Transformations + Actions
b)    Spark SQL → sqlContext.sql + DataFrames → 
c)    Spark Streaming
d)    Accumulators + Broadcast Variables

Deploy Spark → Cluster

a)    Hadoop [ Data Lake ] + Spark as only the processing engine
b)    REPL → Read Evaluate Print Loop -- Interactive mode
c)    spark-submit → Executing this in a batch mode → This is the only way to work when we are talking about Java as a programming language
d)    Notebooks → Python [ Jupyter ] + Zeppelin [ Scala ] 

Basics of Big Data / Hadoop

a)    2003 → Sanjay Ghemawat → What is Hadoop → Doug Cutting → Open sourced Hadoop → 2006 → HDFS [ Hadoop Distributed File System ] 
b)    Map Reduce → Coding in Java
c)    Scripting → Apache Pig
d)    SQL → Apache Hive
e)    NoSQL databases → 
Document    → MongoDB
Columnar    → HBase, Cassandra
Key - Value    → Redis, Riak
Graph        → Neo4J

f)    Apache Sqoop → Move the data from RDBMS to HDFS
g)    Apache Flume → Move data via streaming from twitter, facebook to HDFS

Then we had Apache Spark → 2014 → Open Sources to Apache
Actually was released as a project in 2009 → 

Kafka → Connect to any streaming source    

Hadoop was got 3 versions

a)    Hadoop 0.2x and 1.x
b)    Hadoop 2.x → Oct 15 2013
c)    Hadoop 3 has already come

        Traditional Architecture what we used to follow

    OLTP → RDBMS                OLAP → Data warehouse 

                Hadoop → 
    a)    Structured
    b)    Semi Structured
    c)    UnStructured

Commercial Distribution of Hadoop

a)    Cloudera
b)    Hortonworks → Oct 2018 it is already merged with Cloudera
c)    HDInsight

Cloud
    a)    AWS
    b)    Azure
    c)    GCP

================> Folder Structures in the Image

downloads → Where we will keep our tar files →
lab → Working directory
    data → landing zone in ubuntu where the files to be uploaded to hdfs would be stored
    hdfs → 

    What are the different ways in which we can setup hadoop

a)    Standalone / Pseudo    → Everything would be on our single system
b)    Distributed        → Different components would be on different systems

    What are the different components that is there in Hadoop →

    HDFS [ Storage ]             YARN [ Yet Another Resource Negotiator ] 

    NameNode                Resource Manager
    DataNode                NodetManager
    SecondaryNameNode            JobHistoryServer

    Hadoop is a master slave architecture and since we are having it on one system we are having both the master and slave on the same system.
    
We will have to start with these 6 daemons in our systems. We will start our daemons from home/notroot itself as we have configured it in our .bashrc

NameNode → Details of the Master → metadata → main memory of the system.
DataNode → will contain the actual blocks. 
Secondary NameNode → This is the backup to the Namenode.

If there is a large file 200 MB → The default block size is 128 MB. How many blocks we will have → 2 nos. The benefit is that we can do both reading and writing of data parallely.

If we try to ingest the complete 200 MB in to a single system → it will take some time. 
If we break the data into blocks and then try to ingest it. we will be doing it parallely on 2 systems

In the old time, we stored all the data in HDFS → 16 PB. 

In the recent times, on further analysis, we really do not need the complete 16 PB of data → They only require 30% of the data permanently. The balance 70% will be required for periods of 30, 60, 90 and 120 days. Now the new architecture talks about storing this 70% of the data in Kafka.

https://nosql-database.org/

https://dataconomy.com/2015/08/sql-vs-nosql-vs-newsql-finding-the-right-solution/

https://www.researchgate.net/figure/The-CAP-Theorem-Many-of-the-NOSQL-databases-have-loosened-up-the-requirements-on_fig5_324922396
    


The xml configurations would be present in hadoop/etc/hadoop folder

a)    core-site.xml → IP of the NN
b)    hdfs-site.xml → replication [ default is 3 in a multi node cluster ] , the location of the NN and the location of the DN
c)    yarn-site.xml → default aux-services 
d)    mapred-site.xml → mapreduce, yarn.
        URI
    URL        URN

The default xml file will be of the form of core-default.xml and so on.

a)    start-dfs.sh → HDFS daemons
b)    start-yarn.sh → YARN daemons
c)    notroot@ubuntu:~$ mr-jobhistory-daemon.sh start historyserver

When we do jps → The following daemons will be started :-
notroot@ubuntu:~$ jps
2128 ResourceManager
2242 NodeManager
2578 JobHistoryServer
1939 SecondaryNameNode
2648 Jps
1625 NameNode
1740 DataNode

===========================> Morning Tea Break

Multi-Node Cluster → 4 Nodes → Tell me how the systems would be seperated

Client → outside the cluster

Master        → NN        RM        SNN/JHS

Slave        → DN/NM        DN/NM        DN/NM        DN/NM

JHS can be kept on a separate system or it can be on the same system where the SNN is deployed.

When we deploy a MR application/ Hive / Pig / Sqoop job → at the back, there will always be a Map Reduce Code that will be created. The details of all the Map Reduce code will be stored in the Job History Server page. 

What will happen if the NN goes does down, then we can take the backup from the SNN or we will have a Passive NN which will come up automatically.

The syncing of data between the Active NN and the Passive NN will be real time. 
The syncing of data between the Active NN and the Secondary NN will be done by default in every 1 hour. We can customize the duration.

To get an idea of how the data syncing will happen :- http://hadoop.apache.org/docs/current2/hadoop-project-dist/hadoop-hdfs/HDFSHighAvailabilityWithNFS.html

a)    SNN → only as a backup to the NN and it will not become live when the NN goes down.

b)    In Hadoop 2→ we also have a Passive NN, which will automatically come up when the NN [master ] goes down.

Example of File Ingestion in to HDFS :-

lab/data → txns → put this file in to HDFS →

a)    Create a new directory in HDFS → Call it as /input
        
    hdfs dfs -mkdir /input → This will create a directory in HDFS

    hdfs dfs -ls / → we will see the input directory

    What is another way to see this directory → NN page
    IPADDRESS:50070 → Click on Utilities - Browse the File System

    hdfs dfs → press enter → list of options that we can execute on hadoop.

    execute this from lab/data → hdfs dfs -copyFromLocal txns /input

https://hadoop.apache.org/docs/r2.7.7/hadoop-project-dist/hadoop-hdfs/HDFSCommands.html → This will give detailed options for us.

Where would the files be stored ?  

a)    Look from the NN page and see how the files are distributed into chunks of 128 MB
b)    How to view this in our local FS → 
/home/notroot/lab/hdfs/datanode/current/BP-855377031-127.0.1.1-1547389263682/current/finalized/subdir0/subdir0

To view the data in MB → ls -lh

What is the purpose of the .meta file → It gives us the Checksum Information.

=====================> That is the end of HDFS
Now let us talk about Spark

Spark was created by Matie Zaharia → University of Berkeley → Databricks. → 2009

Processing Engine → features of it → Lightning-fast unified analytics engine

a)    It is an in memory processing engine. It does not store data. Spark can pick up data from

    a)    HDFS
    b)    Local FS
    c)    Databases - Like oracle
    d)    NoSQL → Cassandra, Mongo
    e)    Cloud → S3 / Azure / GCP
    
b)    Faster than the traditional Hadoop

c)    Language of your choice → Scala, Java, Python, R

d)    Deployed anywhere → Standard Hadoop cluster or we can deploy in Cassandra or other NoSQL environment.

e)    Spark can connect to various data sources → 

Example 1: Version of spark that we are using is Spark 2.3.0

a)    Create a simple Java Project with a new package → com.airtel

→ Create a new folder in windows called spark_jars 
→ copy all the jars into the spark_jars folder from the jars directory of spark at this location :- /home/notroot/lab/software/spark-2.3.0-bin-hadoop2.7/jars
→ We will add these jars to the project → Add External jars → Right click on the project - properties - Java Build Path - libraries - add external jars - point to the spark_jars folder where we have all the jars.

Code Walk through

1)    Create a SparkConf to start any standard spark java code.
2)    setMaster is specify the location where the code will be executed. Inside the setMaster we can specify how many cores we can share for running our example. If we say local[*], it will take all the cores. In a real world deployment, we can specify the number of cores that will be used for running our job.It is greedy by nature and will pick up all the cores, if we do not limit it in the setMaster.
3)    If we need to see the spark console, we will have to start the Spark’s History Server.
4)    We will create the JavaSparkContext object

RDD → Resilient Distributed Dataset

3 records → distributed in the 2 cores that we have on the server. So the first core will take 2 records and the second core will take 1 record.

Let us take an example of 200 MB of data

1)    In HDFS, how will the data be stored? 128 MB and 72 MB

2)    When it is loaded in to spark, how will the data be stored? Depends on the number of cores → we have 2 in our example. So it will be 100 MB in each of the cores. 

Hadoop with Map Reduce taken advantage of the Data Locality concept and when we are using spark we do not have this concept at all.

Deployment mode

            Spark
            Hadoop

        Hadoop and Spark in different environments and in that case we will have to load data from hadoop to spark.

=======================>

map operation in spark do → 

Source File                Destination File    
            
10 records        map        10 records

            Will work on all the records.

flatMap → It first does the map logic and then flattens the results. 

In Map Reduce



JavaPairRDD<String, Integer> counts = textFile
        .flatMap(s -> Arrays.asList(s.split("[, ]")).iterator())
        .mapToPair(word -> new Tuple2<>(word, 1))
        .reduceByKey((a, b) -> a + b);

The 3 steps that is flatMap, mapToPair and reduceByKey is what we do in Map Reduce Flow.

The reduceByKey will do both the shuffling and the final reduction.

[“Hello”,(1,1,1,1)] → Hello,4

http://training.databricks.com/visualapi.pdf

File Formats

a)    textFile
b)    csv
c)    JSON
d)    Avro
e)    RC → Row Columnar
f)    Parquet → Custom Format → Reduces the content of the file
        1.48 GB file    → 17 MB
g)    ORC    → Optimized Row Columnar → 1.48 GB → 10 MB
We have seen how to write the code and let us now deploy the same by creating a jar and moving the jar to the programs directory and executing the code. → File - Export - Java - Jar and finish. Then move the jar to programs directory in the cluster and run the jar →

notroot@ubuntu:~/lab/programs$ spark-submit --class com.evenkat.WordCounter Example1.jar

a)    Ensure that we give the path of linux file system → file:///home/notroot/lab/data/words.txt
b)    Change the location in the java code.

In Spark the default location of a file is always HDFS. If you are using the linux file system then we will have to specify file:///

Input File
        JavaRDD<String> textFile = sc.textFile("file:///home/notroot/lab/data/words.txt");
//        JavaRDD<String> textFile = sc.textFile("D:\\SparkWithJava\\data\\words.txt");
//        JavaRDD<String> textFile = sc.textFile("/input/words.txt");

Output Folder
//        counts.saveAsTextFile("D:\\SparkWithJava\\data\\wcount2_java");
        counts.saveAsTextFile("file:///home/notroot/lab/data/words_results.txt");
//        counts.saveAsTextFile("/output/wcount2_java");

Example 2

We printed the individual values in the for loop

Let us create the History Server for Spark :-

What will the HistoryServer do for us: DAG → Directed Acyclic Graph → Run time execution of the codes. → Default location of the DAG will be 4040 → Will not run in Java as the Spark Context is created and closed in the code itself.

Let us start with our History Server

A] Create a directory in HDFS for logging, say /sparkevents

hdfs dfs -mkdir /sparkevents

B] Write down these lines in spark-defaults.conf in the conf directory of spark after removing the template word from this file.

spark.history.fs.logDirectory    hdfs://localhost:9000/sparkevents
spark.eventLog.dir               hdfs://localhost:9000/sparkevents
spark.eventLog.enabled           true

C] from the spark/sbin dir --> ./start-history-server.sh

D] Check with JPS is the HistoryServer daemon is running

E] Execute the spark-submit command again.

F] Then check with the IPADDRESS:18080 port no.
Day 2
Stuff done on Day 1

1)    What is Big Data and Spark
2)    Starting the Hadoop Daemons - HDFS, YARN, JobHistoryServer
3)    File Ingestion into HDFS
4)    What is Spark?
5)     Example 1
6)    Example 2
7)    Starting the Job History Server

==========================>

Different Modes in which a spark cluster will be running

a)    spark-submit
b)    second and third way not being used right now by airtel.

What is required to be there on a system

https://spark.apache.org/docs/latest/cluster-overview.html

Cluster-Mode → Without Hadoop

Components that will be running in a real world environment

a)    Client → Who is going to submit the job → Driver Program
    The client will create a SparkConf and a SparkContext object and through this will submit the job to the ClusterManager. → spark-submit

    In a REPL mode and in a notebook mode, we will not have to create the SparkConf or the SparkContext as it is readily available.

b)    Cluster → Where the job will be running




Cluster Manager → In a multi Node cluster → We will have Master and Slave and the master is called as the cluster manager.

If we have spark and hadoop configured, the Cluster Manager would be ResourceManager for us.
                Driver
        Kafka            Spark            Elastic
        Data            Process        Store
        
                    Consumer
                    Micro-Batch - 2 min

                    Persist            Elastic 

                a)    Reading from Kafka
                b)    Processing the data in Spark
                c)    Writing to Elastic

Streaming → How many components
    Producer
    Consumer → Streaming Code

==================================>

a)    Driver → Spark Context
b)    ClusterManager
c)    Worker Node → Slave
        Cache
            a)    Implicit → System level Cache - Spark Does implicitly.
            b)    Explicit → 
                When you do it, note to uncache the object after it is used at some point of time.
        100 Unit
        Explicit Cache → 40 units
        How much memory will be left for the other jobs: only 60
        Task → Smallest Unit of work that we do. 

        we typically use reference data for explicit cache → 

Let us talk about → other aspects of 4040 or 18080 page →

a)    What is a stage? 

                Core 1            Core 2
DataFile            one two        three
        
flatMap                one two        three
mapToPair            (one,1), (two,1)    (three,1)
coalesce(1)
            Driver → x.coalesce(1)
reduceBuy
2 steps in reduceByKey is shuffle and then reduce.
Data has to be shuffled → whenever there is a shuffling, then there would be a new stage.

reduceByKey            

            Final Output → RDD
            Saving the RDD → In to HDFS, locally in windows, locally in linux
            It will save it in to multiple number of cores that were available. 

By default the number of files will be dependent on the number of cores that we have and incase if you want to have only 1 file we will use an operator called coalesce.

RDD is the complete object → 

        RDD is broken into multiple partitions and those partitions are stored in different cores.

x = sc.parallelize([1,2,3], 2)
y = sc.parallelize([3,4], 1)
z = x.union(y)
        How many partitions will be the final answer in → 3

Every operation would be either a narrow or wide operations.

==============> Post Morning Break

Narrow v/s Wide Transformations? → It is going to use multiple stages or not? 

When a stage is skipped? → 

2 types of RDD

a)    Scalar        : We are only having single values
b)    PairRDD    : We will always have a Key and a Value.

To find out what is available at an individual core → we can use the glom() operator.

To specify the configurations there are places in which we can do that

a)    spark-defaults.conf → partitions → key-value
b)    spark-submit --class XX --master 4 XXX.jar
c)    Within the java code → 

Show the code for rewriting a directory in HDFS and in the local File System.

==============>

groupByKey        and        reduceByKey
When to send the list of        Final Aggregation of the values
values to the downstream.

reduceByKey internally uses the combiner → At the map phase, if there are similar keys, it will combine them and the advantage of this is that

a)    We do not have to worry about the similar keys at the reducing stage. This is because we use the combiner at the map phase which will bring together all the similar values together at the end of the map phase.

x = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])
y = x.groupByKey()
print(x.collect())
print(list((j[0], list(j[1])) for j in y.collect()))

When we talk about spark SQL, we only have DataSets in java.

RDD    → immutable data set which can be used for processing in spark core.
DataSet → tabular view of data → Rows, Columns, Schema by which we can process the data

Catalyst    → 
Tungsten     →    

Spark Core → In python, we need a py4j compiler which will convert the python code in to a byte code.

Spark SQL → All of them are equally fast because of catalyst optimizer.

https://databricks.com/blog/2017/08/31/cost-based-optimizer-in-apache-spark-2-2.html

=========> Post Lunch Break

Example 4 → AirportsInUsaProblem → Page No 5 of our example.
Example 6 → Trying with a Maven Project
Example 8 → Using Filter
Example 10 → Using Union, Intersection, subtract, cartesian
Example 15 → Using Caching - StorageLevel. Memory only

If you want to persist the RDD in HDFS then use the following :-

df.write.save("hdfs://namenode_host:port/file/path");
rdd.saveAsTextFile("hdfs://namenode_host:port/file/path")

Talking about Spark SQL - RDD and DataSet

https://www.cloudera.com/developers/get-started-with-hadoop-tutorial/exercise-1.html

Spark Catalyst → Data Mantra → https://www.youtube.com/watch?v=TCWOJ6EJprY

6)    Example 17 → First Example of Spark SQL
7)    Example 18 → Second Example of Spark SQL
8)    Example 19 → Third Example to reinforce the concepts

What is lazy evaluation in Spark →

We write a simple statement using Transformation → Does it actually execute the code or not? No as only on an action does the code gets evaluated. 

Driver → Component which send the code to the executors
Executor → Component which actually runs the code.

https://academy.datastax.com/resources/getting-started-apache-spark-and-cassandra

Page 38 onwards :- Working with a Semi-Project -> Example 20 and 21 → Datasets would be Durham_County and Wake_County

Example 22: 
Multiline JSON file
    XML file
    Text file
    Avro file
    ORC
    parquet

Working with Accumulator and Broadcast Variable → Example 12,13 and 14

Introduction to ML → Decision Tree and how to work with it.

Job → City, Zip Code → 

=========================> Post Tea Break

When should we use Cache v/s Broadcast Variables

https://unraveldata.com/to-cache-or-not-to-cache/
https://forums.databricks.com/questions/271/should-i-always-cache-my-rdds.html

Cache is normally used when we are working with standalone applications as it is retrieved only in that instance only.

2 stages     → This is the output of the first program
→ If i simply recompute the program

https://jaceklaskowski.gitbooks.io/mastering-spark-sql/spark-sql-Encoder.html

Additional documents are available at this link: https://drive.google.com/open?id=1vyaawOXPX9BlyFM4X0BG5T6cPf00cixs
Day 3
        Hadoop     → Hive [ SQL ]
                → HBase → NoSQL

        Table
        ColumnFamily
        Column

lexicographically sorted
                    emp
        per                    prof
                mobno        
abhinav            567
                987
                123

        telno                    wloc
spenju        123                    delhi

Nitin        123
Nitin        466
Nitin        789

    JP → Hadoop            HBase            CP
                    Cassandra        AP
Use Case → Shopping Use Case

a)    RDBMS
                Transactions → Individual transactions → ACID
                Size → Small size 
                Features → Standard
b)    NoSQL
                                    → CAP
                Size → Huge Size
                Lots of Additional Features 
Time or Account Number
Indexing    →     

If you are having tables in Oracle and move them to hadoop

a)    Hive → General structure 

===================> Accumulators & Broadcast Variables

What is Associative & Commutative

a)    a+b = b+a         → Commutative
b)    a+(b+c) = (a+b)+c    → Associative 

Counters → Statistical information about a job → Map Reduce

ResourceManager → 8088
JobHistoryServer → 19888
SparkHistoryServer → 18080

Spark 1.6 →
            sparkContext
            sqlContext
            streamingContext

Spark 2.0    →     spark → which encapsulates all the above 3 objects.


Example of Working with different Data Structures

Text
Avro → Structure of the data along with the data → 
XML
MultiLine JSON
ORC        → Compress the records → Best form of storing the data
Parquet    → Compress the records and it is the second best form

Kafka → Unique thing that kafka gives us :- Confluent → This has the Schema Registry → This will help us in the schema evolution

Stream1 → Take data from some source, put it in to Kafka → 50 cols
Stream 2 → They need only 6 cols now and if the cols changes we will use Schema registry. Scala as a language in addition to java to work with the Kafka Code.

Node1            Node2            Node3     → Total Capacity is 500 MB

300            300            300

    Another job of 700 MB job comes

If the earlier 300 MB in each machine is running, then we will not be able to execute the 7
700 MB job and it will keep on waiting, till the time the earlier job completes.

To kill a job we will kill by the basis of the Job ID.

============> Try with Avro, Parquet and ORC

================> Basic Example of Streaming.

Let us talk about Machine Learning

Supervised Learning → All the values of some columns known to us
UnSupervised Learning → Some values are not known to us.
ReInforcement Learning

→ Spark ML
→ Graph Based examples → 

https://neo4j.com/blog/analyzing-panama-papers-neo4j/

https://neo4j.com/blog/analyzing-paradise-papers-neo4j/



Training and Testing the Datasets → 
Training 1Dataset → 25%
Testing Dataset → 75%

No of Executors → Physical Systems → 2
In each Executor → we have 2 cores → Total number of cores → 4

https://www.elementsofai.com/

netcat 9999 → This is the way how we configure the producer
Actual Streaming class :-

The minimum time of Time duration of streams would be 20 sec.

So the streaming logic should always be less than the arrival logic.

Source of Data        → Streaming Logic        Persist
                Micro Batch Duration
                
Real time Streaming → Apache Storm, Apache Flink → 


































